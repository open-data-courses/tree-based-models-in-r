---
title: 'Chapter 3: Bagged Trees'
description:
  'In this chapter, you will learn about Bagged Trees, an ensemble method, that uses a combination of trees (instead of only one).'
prev: /chapter2
next: /chapter4
type: chapter
id: 3
---

<exercise id="1" title="Introduction to bagged trees" type="slides">

<slides source="chapter3_01">
</slides>

</exercise>

<exercise id="2" title="Advantages of bagged trees">

What are the advantages of bagged trees compared to a single tree?

<choice>
<opt text="Increases the accuracy of the resulting predictions">

This is true, but there's another true statement here as well.

</opt>

<opt text="Easier to interpret the resulting model" >

Bagging improves prediction accuracy at the expense of interpretability.

</opt>

<opt text="Reduces variance by averaging a set of observations">

This is true, but there's another true statement here as well.

</opt>

<opt text="1 and 2 are correct">

1 is true, but 2 is not true.

</opt>

<opt text="1 and 3 are correct" correct="true">

Well done. Proceed to the next exercise.

</opt>

<opt text="2 and 3 are correct">

3 is true, but 2 is not true.

</opt>
</choice>

</exercise>

<exercise id="3" title="Build a classification tree">

Let's start by training a bagged tree model. You'll be using the `bagging()` function from the `ipred` package. The number of bagged trees can be specified using the `nbagg` parameter, but here we will use the default (25).

If we want to estimate the model's accuracy using the "out-of-bag" (OOB) samples, we can set the the `coob` parameter to `TRUE`.  The OOB samples are the training obsevations that were not selected into the bootstrapped sample (used in training).  Since these observations were not used in training, we can use them instead to evaluate the accuracy of the model (done automatically inside the `bagging()` function).

**Instructions**

The data frame `creditsub` is in the workspace.  This data frame is a subset of the original German Credit Dataset, which we will use to train our first classification tree model.

- The `credit_train` and `credit_test` datasets from Chapter 1 are already loaded in the workspace.
- Use the `bagging()` function to train a bagged tree model.
- Inspect the model by printing it.

<codeblock id="03_03">

You will need to specify the training dataset for the `data` argument.

</codeblock>

</exercise>

<exercise id="4" title="Evaluating the bagged tree performance" type="slides">

<slides source="chapter3_04">
</slides>

</exercise>

<exercise id="5" title="Prediction and confusion matrix">

As you saw in the video, a confusion matrix is a very useful tool for examining all possible outcomes of your predictions (true positive, true negative, false positive, false negative).

In this exercise, you will predict those who will default using bagged trees. You will also create the confusion matrix using the `confusionMatrix()` function from the **caret** package. 

It's always good to take a look at the output using the `print()` function.

**Instructions**

The fitted model object, `credit_model`, is already in your workspace.

- Use the `predict()` function with `type = "class"` to generate predicted labels on the `credit_test` dataset. 
- Take a look at the prediction using the `print()` function.
- Calculate the confusion matrix using the `confusionMatrix` function.

<codeblock id="03_05">

The `predict()` function for a bagged tree object works the same as the `predict()` function for an `rpart` model.  The `confusionMatrix()` function takes as input, the predicted classes that you'll generate, and the actual classes, which are stored in `credit_test$default`.

</codeblock>

</exercise>

<exercise id="6" title="Predict on a test set and compute AUC">

In binary classification problems, we can predict numeric values instead of class labels.  In fact, class labels are created only after you use the model to predict a raw, numeric, _predicted value_ for a test point.  

The _predicted label_ is generated by applying a threshold to the _predicted value_, such that all tests points with predicted value greater than that threshold get a predicted label of "1" and, points below that threshold get a predicted label of "0".

In this exercise, generate predicted values (rather than class labels) on the test set and evaluate performance based on [AUC (Area Under the ROC Curve)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve).  The AUC is a common metric for evaluating the discriminatory ability of a binary classification model.

**Instructions**

- Use the `predict()` function with `type = "prob"` to generate numeric predictions on the `credit_test` dataset. 
- Compute the AUC using the `auc()` function from the **Metrics** package.

<codeblock id="03_06">

Apply the `predict()` function to `credit_model` object and `credit_test` dataset.

</codeblock>

</exercise>

<exercise id="7" title="Using caret for cross-validating models" type="slides">

<slides source="chapter3_07">
</slides>

</exercise>

<exercise id="8" title="Cross-validate a bagged tree model in caret">

Use `caret::train()` with the `"treebag"` method to train a model and evaluate the model using cross-validated AUC.  The **caret** package allows the user to easily cross-validate any model across any relevant performance metric.  In this case, we will use 5-fold cross validation and evaluate cross-validated AUC (Area Under the ROC Curve). 

**Instructions**

The `credit_train` dataset is in your workspace.  You will use this data frame as the training data.

- First specify a `ctrl` object, which is created using the `caret::trainControl()` function.  
- In the `trainControl()` function, you can specify many things.  We will set: `method = "cv"`, `nfolds = 5` for 5-fold cross-validation. Also, two options that are required if you want to use AUC as the metric: `classProbs = TRUE` and `summaryFunction = twoClassSummary`.

<codeblock id="03_08">

The `trainControl()` function requires a number of folds, and here we'd like to use 5.  The `train()` function requires the training set and also the `ctrl` object.

</codeblock>

</exercise>

<exercise id="09" title="Generate predictions from the caret model">

Generate predictions on a test set for the `caret` model.

**Instructions**

- First generate predictions on the `credit_test` data frame using the `credit_caret_model` object.  
- After generating test set predictions, use the `auc()` function from the `Metrics` package to compute AUC.  
 
<codeblock id="03_09">

- Pass the `credit_caret_model` and `credit_test` objects to the `predict()` function.  Once you have those predictions, use them in the `auc()` function.
- The value for `actual` you should recode is stored in `credit_test$default`.

</codeblock>

</exercise>

<exercise id="10" title="Compare test set performance to CV performance">

In this excercise, you will print test set AUC estimates that you computed in previous exercises.  These two methods use the same code underneath, so the estimates should be very similar.

- The `credit_ipred_model_test_auc` object stores the test set AUC from the model trained using the `ipred::bagging()` function.
- The `credit_caret_model_test_auc` object stores the test set AUC from the model trained using the `caret::train()` function with `method = "treebag"`.

Lastly, we will print the 5-fold cross-validated estimate of AUC that is stored within the `credit_caret_model` object.  This number will be a more accurate estimate of the true model performance since we have averaged the performance over five models instead of just one.  

On small datasets like this one, the difference between test set model performance estimates and cross-validated model performance estimates will tend to be more pronounced.  When using small data, it's recommended to use cross-validated estimates of performance because they are more stable.

**Instructions**

- Print the object `credit_ipred_model_test_auc`.
- Print the object `credit_caret_model_test_auc`.
- Compare these to the 5-fold cross validated AUC.

<codeblock id="03_10">

The cross-validated AUC value is stored in the `"ROC"` column of the results table inside a **caret** model.

</codeblock>

</exercise>


